
Una vez obtenida toda la informaci\'on posible del mu\'on a su paso por CMS, el objetivo es entrenar una red neuronal profunda tomando como entrada la informaci\'on de la traza en el tracker y la informaci\'on espacial de las se\~nales recogidas en el sistema de muones, como el n\'umero de hits y la desviaci\'on standard espacial en cada estaci\'on. En este caso, la funci\'on de p\'erdida a minimizar ser\'a  una funci\'on que dependa de la diferencia entre el $p_{T}$ de generaci\'on y el $p_{T}$ que se quiere predecir, para as\'i aprender las caracter\'isticas de los muones y hacer regresi\'on al momento transverso reconstruido. \\

Para el entrenamiento se utilizar\'a la librer\'ia de Python de c\'odigo abierto Keras~\cite{chollet2015keras}, que se caracteriza principalmente por ofrecer sencillez de uso para el usuario, y la red se ejecuta sobre TensorFlow~\cite{tensorflow2015-whitepaper}.


\subsection{Variables de entrenamiento}\label{sec:variables}

Las variables utilizadas para el entrenamiento se dividen en dos categor\'ias seg\'un su naturaleza:

\begin{itemize}
\item Caracter\'sticas de la traza interna reconstruida en el tracker: $p_{T}$, $\eta$, $\phi$, carga.
\item Informaci\'on del conjunto de segmentos recogidos en cada estaci\'on de DTs y CSCs: n\'umero total de segmentos en la estaci\'on, media espacial en x, y, z, desviaci\'on est\'andard en x, y, z, asimetr\'ia en x, y, z, kurtosis en x, y, z.
\end{itemize}

En la Figura~\ref{fig:train_vars} se muestran las distribuciones de las variables provenientes de la traza del tracker y de las estaciones de las DTs.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{figures/Training_vars_2.png}
\caption{Distribuciones de las variables de entrenamiento}
\label{fig:train_vars}        
\end{figure}


\subsection{Conjunto de datos y arquitectura de la red}\label{sec:arch}

El conjunto de datos, compuesto por un total de 195777 muones, se ha dividido en un 80\% para el entrenamiento del modelo (del que un 10\% ser\'a usado como dataset de validaci\'on), mientras que un 20\% es utilizado para el testeo del mismo. \\

En cuanto al tipo de red utilizada, se ha entrenado una red de tipo fully-conected donde todos los nodos de cada capa est\'an conectados con los nodos de las capas contiguas, con la siguiente arquitectura:


\begin{itemize}

\item Activaci\'on: ReLu~\cite{agarap2018deep} en las capas 1-9; Lineal en la capa 10 para hacer regresi\'on al $p_{T}$.

\item Optimizador: Adam (lr=0.0005)~\cite{Kingma2015AdamAM}.

\item Funci\'on de p\'erdida: Mean Square Error.

\item \'Epocas: 1000.

\item Batch size: 2000.

\item Early stopping en la p\'erdida del dataset de validaci\'on con paciencia de 50 \'epocas. De esta manera, para cada \'epoca se obtiene el valor del MSE en el conjunto de datos de validaci\'on (que no se utilizan para entrenar), y se guarda el modelo con menor MSE en validaci\'on. El entrenamiento se detinene si el MSE no ha mejorado en 50 \'epocas despu\'es del valor m\'inimo encontrado, evitando as\'i que el modelo sobreentrene.

\end{itemize}


\subsection{Resultados del entrenamiento}\label{sec:trainresults}

En la Figura~\ref{fig:model_loss} se muestra la el valor de la p\'erdida del MSE en funci\'on de la \'epoca de entrenamiento. El mejor modelo de obtiene despu\'es de 184 \'epocas, con un valor de p\'erdida para el conjunto de datos de entrenamiento de 49605.3, para el conjunto de datos de validaci\'on de 50399.8, y para el conjunto de datos de testeo de 53731.6.  \\

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{figures/model_loss.png}
\caption{Valor del MSE en funci\'on de la \'epoca de entrenamiento para el conjunto de datos de entrenamiento (en azul) y para el conjunto de datos de validaci\'on (en naranja).}
\label{fig:model_loss}        
\end{figure}


En la Figura~\ref{fig:R_predicted} se comparan la distribuciones de la resoluci\'on R, definida en \eqref{eq:R}, calculadas sobre el conjunto de datos de testeo a partir del $p_{T}$ que da el algoritmo TuneP y a partir del $p_{T}$ que predice la red neuronal respectivamente.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{figures/R_predicted.png}
\caption{Distribuci\'on de la resoluci\'on R sobre el conjunto de datos de testeo. Izquierda: tomando el $p_{T}$ que da el algoritmo TuneP. Derecha: tomando el $p_{T}$ predicho por el modelo de regresi\'on al $p_{T}$.}
\label{fig:R_predicted}        
\end{figure}

